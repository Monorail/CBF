import java.util.ArrayList;
import java.util.Random;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

public class BloomFilter<AnyType> implements Runnable {

	static int numThreads = 4;
	static ArrayList<Thread> threads = new ArrayList<Thread>();
	static ArrayList<BloomFilter<String>> workers = new ArrayList<BloomFilter<String>>();
	static hashSet[] hashTables;
	static int tableSize = 53;
	static int numTables = 5;
	static AtomicInteger finishedThreads = new AtomicInteger(0);
	static AtomicBoolean[] announceTable;

	// set up wait free queue for adding elements
	static ConcurrentLinkedQueue<String> opQueue = new ConcurrentLinkedQueue<String>();

	public static void main(String[] args) {
		// sample strings
		String[] filterStrs = { "orange", "baseball", "apple", "football",
				"nuclear", "prairie", "niner", "backgammon" };

		// prevent nonconcurrent executions
		if (numThreads <= 1) {
			System.out.println("You can't use <= one thread");
			return;
		}

		// currently using varying table sizes rather than multiple hash
		// functions
		// ***Intend to change***

		// initialize hash table and capacity array
		hashTables = new hashSet[numTables];
		announceTable = new AtomicBoolean[numTables];
		for (int i = 0; i < numTables; i++) {
			hashTables[i] = new hashSet(tableSize, i);
			announceTable[i] = new AtomicBoolean(false);
		}

		// generate 40 operations at random to perform
		Random R = new Random();
		for (int i = 0; i < 40; i++) {
			Boolean opAdd = R.nextBoolean();
			String operation = opAdd ? "add " : "con ";

			int strsIndex = R.nextInt(filterStrs.length);
			operation += filterStrs[strsIndex];

			// for(int j = 1+R.nextInt(11); j >=0; j--){
			// char derp = (char) (R.nextInt(26) + 'a');
			// operation += derp;
			// }

			// add operations to wait-free queue
			opQueue.add(operation);

			System.out.println(operation);
		}

		// initialize threads
		for (int i = 0; i < numThreads; i++) {
			BloomFilter<String> temp = new BloomFilter<String>(i);
			workers.add(temp);
			threads.add(new Thread(temp));
		}

		// start all threads
		for (int i = 0; i < numThreads; i++) {
			threads.get(i).start();
		}

	}

	int ID;

	public BloomFilter(int ID) {
		this.ID = ID;
	}

	@Override
	public void run() {
		String operation = "";
		// poll wait-free queue for an operation until queue is empty
		while ((operation = opQueue.poll()) != null) {
			// System.out.println(ID + " " + operation);

			// parse the operation
			String[] ops = operation.split(" ");
			// call contains method
			if (ops[0].equals("con")) {
				System.out.printf("The bloom filter %s the name %s\n",
						contains((AnyType) ops[1]) ? "may contain"
								: "does not contain", ops[1]);
			}
			// call add method
			else {
				add((AnyType) ops[1]);
				System.out.println("Added " + ops[1]);
			}
		}
	}

	// adds object to the filter
	public void add(AnyType o) {
		// generate hash code for the object
		int hval = o.hashCode();

		// check for indexOutOfBounds error
		if (hval < 0)
			hval = hval * -1;

		// mark bits that correspond to hash value in each hash set as true
		for (int i = 0; i < hashTables.length; i++) {
			hashTables[i].add(MurmurHash.hashItUp(o, i) % tableSize);
			// hashTables[i].add(hval % tableSize);
		}
	}

	// checks for object inside the filter
	public boolean contains(AnyType o) {
		// generate hash code for the object
		int hval = o.hashCode();

		// resolve indexOutOfBoundsError
		if (hval < 0)
			hval = -hval;

		// check bits that correspond to hash value in each hash set
		// if every bit is marked, the filter MAY contain the object
		for (int i = 0; i < hashTables.length; i++)
			if (hashTables[i].contains(hval % tableSize))
				return false;

		return true;
	}

	static class hashSet {
		hashSet next = null;
		int tableSize, ID;
		AtomicInteger capacity;
		boolean full = false;
		boolean[] set;

		public hashSet(int size, int ID) {
			this.tableSize = size;
			this.ID = ID;
			capacity = new AtomicInteger(0);
			set = new boolean[tableSize];
		}

		public boolean contains(int index) {
			System.out.println(index + " " + tableSize);
			return (next != null) ? set[index % tableSize] : next.contains(index%index % tableSize);
		}

		public void add(int index) {
			if (!full) {
				// increment number true if change
				if (!set[index % tableSize]){
					capacity.incrementAndGet();
					full = (((float) capacity.get()) / tableSize) > (.5f);
				}
				set[index % tableSize] = true;
			} else { // next is not being created
				if (announceTable[ID].compareAndSet(false, true)) {
					next = new hashSet(tableSize, ID);
					announceTable[ID].compareAndSet(true, false);
				} else {
					while (announceTable[ID].get());
					next.add(index);
				}

			}
		}

	}
}

/*
 * hash set capacity:
 * 
 * \subsection{Ways that we combat concurrency problems} Although we have not
 * devised any solutions to these problems yet, these are the main problems that
 * we have currently theorized may complicate the wait-free overflow Bloom
 * filter.
 * 
 * \subsubsection{Dynamic Creation of Additional Filters} A thread must take the
 * burden of creating the overflow filters so that multiple threads do not try
 * to create the same resource simultaneously. We think that an announcement
 * table should be implemented for this purpose to ensure that two threads do
 * not both try to access the same resource simultaneously.
 * 
 * \subsubsection{Capacity Checking} While a thread is creating an additional
 * Bloom filter for a hash set that has reached capacity, the operation of other
 * threads must be halted to guarantee that more elements are not added to that
 * hash set. We believe there are three potential solutions to this problem. The
 * first of the three options is to halt the addition of more elements while the
 * overflow filter is being created. This is most likely the worst as it
 * destroys the wait-free property and is also very slow. The second is to have
 * threads create the buffers before they are needed to be used. If the creation
 * operation takes too long or too many buffers need to be created at once
 * however, the program may need to halt to catch up. Finally, the guarantee of
 * the filter can be relaxed slightly to ensure that the error rate is around a
 * certain percent most of the time, but may be slightly higher sometimes. If a
 * buffer creation takes too long, additional elements will be added to the
 * “full” one which will increase the error rate slightly. We believe right now
 * that the best solution is a combination of methods two and three.
 * 
 * \subsubsection{Record Keeping} The capacity of the individual hash sets must
 * be kept track of at all times to ensure that the overflow filter is created
 * while maintaining the false positive guarantee. If each thread were to
 * calculate the current fullness each time an element is added, it may cause an
 * unneeded slowdown as the check turns an O(1) operation into an O(n)
 * operation, as well as being inaccurate as other threads may be changing
 * elements in the array as others are doing calculations. This may not matter
 * depending on how the linearization point for the operation is defined but it
 * is slow none the less. An alternative method is to keep a shared counter
 * keeping track of the number of elements that have been changed in each hash
 * set, however this will lead to the usage of highly contested resources, which
 * is also undesirable.
 */

